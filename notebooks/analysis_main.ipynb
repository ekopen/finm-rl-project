{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# FINM RL Trading - Main Analysis Notebook\n",
        "\n",
        "This notebook provides comprehensive analysis of all experimental results:\n",
        "- Core baselines comparison (PPO vs BuyAndHold vs MACrossover)\n",
        "- Hyperparameter sensitivity analysis\n",
        "- State/Environment ablation studies\n",
        "- Regime-based performance analysis\n",
        "- Robustness testing results\n",
        "- Summary dashboard\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup: Paths and Imports\n",
        "import sys\n",
        "from pathlib import Path\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from typing import Dict, Any, List\n",
        "\n",
        "# Set up paths\n",
        "CURRENT_DIR = Path().resolve()\n",
        "PROJECT_ROOT = CURRENT_DIR.parent\n",
        "RESULTS_DIR = PROJECT_ROOT / \"results\"\n",
        "\n",
        "if str(PROJECT_ROOT) not in sys.path:\n",
        "    sys.path.insert(0, str(PROJECT_ROOT))\n",
        "\n",
        "print(f\"PROJECT_ROOT = {PROJECT_ROOT}\")\n",
        "print(f\"RESULTS_DIR = {RESULTS_DIR}\")\n",
        "print(f\"Results available: {list(RESULTS_DIR.iterdir()) if RESULTS_DIR.exists() else 'Not found'}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configure plotting style\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "sns.set_palette(\"husl\")\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n",
        "plt.rcParams['font.size'] = 10\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Export utilities\n",
        "def export_table_csv(df: pd.DataFrame, path: Path) -> None:\n",
        "    \"\"\"Export DataFrame to CSV.\"\"\"\n",
        "    df.to_csv(path, index=False)\n",
        "    print(f\"Exported table to {path}\")\n",
        "\n",
        "def export_table_latex(df: pd.DataFrame, path: Path, **kwargs) -> None:\n",
        "    \"\"\"Export DataFrame to LaTeX format.\"\"\"\n",
        "    latex_str = df.to_latex(index=False, **kwargs)\n",
        "    with open(path, 'w') as f:\n",
        "        f.write(latex_str)\n",
        "    print(f\"Exported LaTeX table to {path}\")\n",
        "\n",
        "def export_table_html(df: pd.DataFrame, path: Path, **kwargs) -> None:\n",
        "    \"\"\"Export DataFrame to HTML format.\"\"\"\n",
        "    html_str = df.to_html(index=False, **kwargs)\n",
        "    with open(path, 'w') as f:\n",
        "        f.write(html_str)\n",
        "    print(f\"Exported HTML table to {path}\")\n",
        "\n",
        "# Create exports directory\n",
        "EXPORTS_DIR = PROJECT_ROOT / \"notebooks\" / \"exports\"\n",
        "EXPORTS_DIR.mkdir(exist_ok=True)\n",
        "print(f\"Export directory: {EXPORTS_DIR}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Core Baselines Comparison\n",
        "\n",
        "Compare PPO performance against simple baselines: BuyAndHold and MACrossover.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Load core baselines metrics\n",
        "core_baselines_path = RESULTS_DIR / \"core_baselines\" / \"core_metrics.json\"\n",
        "with open(core_baselines_path, 'r') as f:\n",
        "    core_metrics = json.load(f)\n",
        "\n",
        "# Convert to DataFrame for easier analysis\n",
        "baselines_data = []\n",
        "for strategy, metrics in core_metrics.items():\n",
        "    row = {\"strategy\": strategy}\n",
        "    row.update(metrics)\n",
        "    baselines_data.append(row)\n",
        "\n",
        "baselines_df = pd.DataFrame(baselines_data)\n",
        "print(\"Core Baselines Comparison:\")\n",
        "print(baselines_df.to_string(index=False))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Visualize baselines comparison\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "\n",
        "metrics_to_plot = ['total_return', 'annualized_return', 'sharpe', 'max_drawdown']\n",
        "for idx, metric in enumerate(metrics_to_plot):\n",
        "    ax = axes[idx // 2, idx % 2]\n",
        "    bars = ax.bar(baselines_df['strategy'], baselines_df[metric])\n",
        "    ax.set_title(f'{metric.replace(\"_\", \" \").title()}')\n",
        "    ax.set_ylabel(metric.replace(\"_\", \" \").title())\n",
        "    ax.tick_params(axis='x', rotation=45)\n",
        "    \n",
        "    # Add value labels on bars\n",
        "    for bar in bars:\n",
        "        height = bar.get_height()\n",
        "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
        "                f'{height:.3f}',\n",
        "                ha='center', va='bottom')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(EXPORTS_DIR / \"baselines_comparison.png\", dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "print(f\"Saved baselines comparison plot to {EXPORTS_DIR / 'baselines_comparison.png'}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Export baselines comparison table\n",
        "export_table_csv(baselines_df, EXPORTS_DIR / \"baselines_comparison.csv\")\n",
        "export_table_latex(baselines_df, EXPORTS_DIR / \"baselines_comparison.tex\", float_format=\"%.4f\")\n",
        "export_table_html(baselines_df, EXPORTS_DIR / \"baselines_comparison.html\", classes='table table-striped')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Summary Dashboard\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load hyperparameter results\n",
        "hyperparams_dir = RESULTS_DIR / \"ppo_hyperparams\"\n",
        "hyperparams_summary_path = hyperparams_dir / \"summary_test_metrics.csv\"\n",
        "\n",
        "if hyperparams_summary_path.exists():\n",
        "    hyperparams_df = pd.read_csv(hyperparams_summary_path)\n",
        "    print(\"Hyperparameter Summary:\")\n",
        "    print(hyperparams_df.to_string(index=False))\n",
        "    \n",
        "    # Load individual run JSONs to extract config details\n",
        "    run_configs = []\n",
        "    for json_file in hyperparams_dir.glob(\"*.json\"):\n",
        "        if json_file.name != \"summary_test_metrics.json\":\n",
        "            with open(json_file, 'r') as f:\n",
        "                run_data = json.load(f)\n",
        "                if 'config' in run_data:\n",
        "                    config = run_data['config']\n",
        "                    name = run_data.get('name', json_file.stem)\n",
        "                    metrics = run_data.get('test_metrics', {})\n",
        "                    run_configs.append({\n",
        "                        'name': name,\n",
        "                        'clip_epsilon': config.get('clip_epsilon', None),\n",
        "                        'entropy_coef': config.get('entropy_coef', None),\n",
        "                        'gamma': config.get('gamma', None),\n",
        "                        'gae_lambda': config.get('gae_lambda', None),\n",
        "                        **metrics\n",
        "                    })\n",
        "    \n",
        "    if run_configs:\n",
        "        hyperparams_detailed_df = pd.DataFrame(run_configs)\n",
        "        print(\"\\nDetailed Hyperparameter Configurations:\")\n",
        "        print(hyperparams_detailed_df.to_string(index=False))\n",
        "else:\n",
        "    print(\"Hyperparameter results not found\")\n",
        "    hyperparams_df = None\n",
        "    hyperparams_detailed_df = None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create hyperparameter sensitivity plots\n",
        "if hyperparams_df is not None and hyperparams_detailed_df is not None:\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "    \n",
        "    # Plot 1: Sharpe vs Clip Epsilon\n",
        "    if 'clip_epsilon' in hyperparams_detailed_df.columns:\n",
        "        ax = axes[0, 0]\n",
        "        clip_data = hyperparams_detailed_df[hyperparams_detailed_df['clip_epsilon'].notna()]\n",
        "        if not clip_data.empty:\n",
        "            ax.scatter(clip_data['clip_epsilon'], clip_data['sharpe'], s=100, alpha=0.7)\n",
        "            ax.set_xlabel('Clip Epsilon')\n",
        "            ax.set_ylabel('Sharpe Ratio')\n",
        "            ax.set_title('Sharpe Ratio vs Clip Epsilon')\n",
        "            ax.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Plot 2: Sharpe vs Entropy Coefficient\n",
        "    if 'entropy_coef' in hyperparams_detailed_df.columns:\n",
        "        ax = axes[0, 1]\n",
        "        ent_data = hyperparams_detailed_df[hyperparams_detailed_df['entropy_coef'].notna()]\n",
        "        if not ent_data.empty:\n",
        "            ax.scatter(ent_data['entropy_coef'], ent_data['sharpe'], s=100, alpha=0.7)\n",
        "            ax.set_xlabel('Entropy Coefficient')\n",
        "            ax.set_ylabel('Sharpe Ratio')\n",
        "            ax.set_title('Sharpe Ratio vs Entropy Coefficient')\n",
        "            ax.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Plot 3: Total Return by Configuration\n",
        "    ax = axes[1, 0]\n",
        "    hyperparams_df_sorted = hyperparams_df.sort_values('total_return', ascending=False)\n",
        "    bars = ax.barh(hyperparams_df_sorted['name'], hyperparams_df_sorted['total_return'])\n",
        "    ax.set_xlabel('Total Return')\n",
        "    ax.set_title('Total Return by Hyperparameter Configuration')\n",
        "    ax.grid(True, alpha=0.3, axis='x')\n",
        "    \n",
        "    # Plot 4: Sharpe by Configuration\n",
        "    ax = axes[1, 1]\n",
        "    hyperparams_df_sorted_sharpe = hyperparams_df.sort_values('sharpe', ascending=False)\n",
        "    bars = ax.barh(hyperparams_df_sorted_sharpe['name'], hyperparams_df_sorted_sharpe['sharpe'])\n",
        "    ax.set_xlabel('Sharpe Ratio')\n",
        "    ax.set_title('Sharpe Ratio by Hyperparameter Configuration')\n",
        "    ax.grid(True, alpha=0.3, axis='x')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig(EXPORTS_DIR / \"hyperparams_sensitivity.png\", dpi=150, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    print(f\"Saved hyperparameter sensitivity plot to {EXPORTS_DIR / 'hyperparams_sensitivity.png'}\")\n",
        "    \n",
        "    # Highlight best/worst\n",
        "    print(\"\\nBest Configuration (by Sharpe):\")\n",
        "    best_sharpe = hyperparams_df.loc[hyperparams_df['sharpe'].idxmax()]\n",
        "    print(best_sharpe)\n",
        "    \n",
        "    print(\"\\nWorst Configuration (by Sharpe):\")\n",
        "    worst_sharpe = hyperparams_df.loc[hyperparams_df['sharpe'].idxmin()]\n",
        "    print(worst_sharpe)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Export hyperparameter tables\n",
        "if hyperparams_df is not None:\n",
        "    export_table_csv(hyperparams_df, EXPORTS_DIR / \"hyperparams_summary.csv\")\n",
        "    export_table_latex(hyperparams_df, EXPORTS_DIR / \"hyperparams_summary.tex\", float_format=\"%.4f\")\n",
        "    \n",
        "if hyperparams_detailed_df is not None:\n",
        "    export_table_csv(hyperparams_detailed_df, EXPORTS_DIR / \"hyperparams_detailed.csv\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. State/Environment Ablation Analysis\n",
        "\n",
        "Compare performance across different state representations and environment configurations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load state/env ablation results\n",
        "states_envs_dir = RESULTS_DIR / \"states_envs\"\n",
        "states_envs_data = []\n",
        "\n",
        "for json_file in states_envs_dir.glob(\"*_metrics.json\"):\n",
        "    with open(json_file, 'r') as f:\n",
        "        metrics = json.load(f)\n",
        "        name = json_file.stem.replace(\"_metrics\", \"\")\n",
        "        row = {\"config\": name}\n",
        "        # Remove shaping_config if present\n",
        "        if 'shaping_config' in metrics:\n",
        "            del metrics['shaping_config']\n",
        "        row.update(metrics)\n",
        "        states_envs_data.append(row)\n",
        "\n",
        "if states_envs_data:\n",
        "    states_envs_df = pd.DataFrame(states_envs_data)\n",
        "    print(\"State/Environment Ablation Results:\")\n",
        "    print(states_envs_df.to_string(index=False))\n",
        "    \n",
        "    # Visualize comparison\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "    metrics_to_plot = ['total_return', 'annualized_return', 'sharpe', 'max_drawdown']\n",
        "    \n",
        "    for idx, metric in enumerate(metrics_to_plot):\n",
        "        ax = axes[idx // 2, idx % 2]\n",
        "        bars = ax.bar(states_envs_df['config'], states_envs_df[metric])\n",
        "        ax.set_title(f'{metric.replace(\"_\", \" \").title()}')\n",
        "        ax.set_ylabel(metric.replace(\"_\", \" \").title())\n",
        "        ax.tick_params(axis='x', rotation=45)\n",
        "        \n",
        "        # Add value labels\n",
        "        for bar in bars:\n",
        "            height = bar.get_height()\n",
        "            ax.text(bar.get_x() + bar.get_width()/2., height,\n",
        "                    f'{height:.3f}',\n",
        "                    ha='center', va='bottom')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig(EXPORTS_DIR / \"states_envs_comparison.png\", dpi=150, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    print(f\"Saved state/env comparison plot to {EXPORTS_DIR / 'states_envs_comparison.png'}\")\n",
        "    \n",
        "    # Export table\n",
        "    export_table_csv(states_envs_df, EXPORTS_DIR / \"states_envs_comparison.csv\")\n",
        "    export_table_latex(states_envs_df, EXPORTS_DIR / \"states_envs_comparison.tex\", float_format=\"%.4f\")\n",
        "else:\n",
        "    print(\"State/Environment results not found\")\n",
        "    states_envs_df = None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load robustness results\n",
        "robustness_dir = RESULTS_DIR / \"ppo_seed_pretrain_compare\"\n",
        "robustness_summary_path = robustness_dir / \"summary_test_metrics.csv\"\n",
        "robustness_group_stats_path = robustness_dir / \"summary_group_stats.csv\"\n",
        "\n",
        "if robustness_summary_path.exists():\n",
        "    from eval.summarize import summarize_runs\n",
        "    \n",
        "    # Load individual run JSONs\n",
        "    run_payloads = []\n",
        "    for json_file in robustness_dir.glob(\"*.json\"):\n",
        "        if json_file.name not in [\"summary_test_metrics.json\"]:\n",
        "            with open(json_file, 'r') as f:\n",
        "                run_payloads.append(json.load(f))\n",
        "    \n",
        "    if run_payloads:\n",
        "        robustness_df = summarize_runs(run_payloads, metric_key=\"test_metrics\")\n",
        "        print(\"Robustness Summary (per run):\")\n",
        "        print(robustness_df.to_string(index=False))\n",
        "        \n",
        "        # Load group statistics\n",
        "        if robustness_group_stats_path.exists():\n",
        "            group_stats_df = pd.read_csv(robustness_group_stats_path)\n",
        "            print(\"\\nGroup Statistics (mean/std by pretrained vs non-pretrained):\")\n",
        "            print(group_stats_df.to_string(index=False))\n",
        "            \n",
        "            # Visualize group statistics\n",
        "            fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "            metrics = ['total_return', 'annualized_return', 'sharpe', 'max_drawdown']\n",
        "            \n",
        "            for idx, metric in enumerate(metrics):\n",
        "                ax = axes[idx // 2, idx % 2]\n",
        "                metric_data = group_stats_df[group_stats_df['metric'] == metric]\n",
        "                \n",
        "                groups = metric_data['group'].unique()\n",
        "                means = [metric_data[metric_data['group'] == g]['mean'].values[0] for g in groups]\n",
        "                stds = [metric_data[metric_data['group'] == g]['std'].values[0] for g in groups]\n",
        "                \n",
        "                x_pos = np.arange(len(groups))\n",
        "                bars = ax.bar(x_pos, means, yerr=stds, capsize=5, alpha=0.7)\n",
        "                ax.set_xticks(x_pos)\n",
        "                ax.set_xticklabels(groups)\n",
        "                ax.set_ylabel(metric.replace(\"_\", \" \").title())\n",
        "                ax.set_title(f'{metric.replace(\"_\", \" \").title()} by Group (mean Â± std)')\n",
        "                ax.grid(True, alpha=0.3, axis='y')\n",
        "            \n",
        "            plt.tight_layout()\n",
        "            plt.savefig(EXPORTS_DIR / \"robustness_comparison.png\", dpi=150, bbox_inches='tight')\n",
        "            plt.show()\n",
        "            print(f\"Saved robustness comparison plot to {EXPORTS_DIR / 'robustness_comparison.png'}\")\n",
        "            \n",
        "            # Export tables\n",
        "            export_table_csv(group_stats_df, EXPORTS_DIR / \"robustness_group_stats.csv\")\n",
        "            export_table_latex(group_stats_df, EXPORTS_DIR / \"robustness_group_stats.tex\", float_format=\"%.4f\")\n",
        "    else:\n",
        "        print(\"No robustness run data found\")\n",
        "        robustness_df = None\n",
        "        group_stats_df = None\n",
        "else:\n",
        "    print(\"Robustness results not found\")\n",
        "    robustness_df = None\n",
        "    group_stats_df = None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create master comparison table\n",
        "master_data = []\n",
        "\n",
        "# Add baselines\n",
        "if 'baselines_df' in locals() and baselines_df is not None:\n",
        "    for _, row in baselines_df.iterrows():\n",
        "        master_data.append({\n",
        "            'experiment_type': 'baseline',\n",
        "            'name': row['strategy'],\n",
        "            'total_return': row['total_return'],\n",
        "            'annualized_return': row['annualized_return'],\n",
        "            'sharpe': row['sharpe'],\n",
        "            'max_drawdown': row['max_drawdown']\n",
        "        })\n",
        "\n",
        "# Add best hyperparameter config\n",
        "if 'hyperparams_df' in locals() and hyperparams_df is not None:\n",
        "    best_hyperparam = hyperparams_df.loc[hyperparams_df['sharpe'].idxmax()]\n",
        "    master_data.append({\n",
        "        'experiment_type': 'hyperparams',\n",
        "        'name': f\"Best HP: {best_hyperparam['name']}\",\n",
        "        'total_return': best_hyperparam['total_return'],\n",
        "        'annualized_return': best_hyperparam['annualized_return'],\n",
        "        'sharpe': best_hyperparam['sharpe'],\n",
        "        'max_drawdown': best_hyperparam['max_drawdown']\n",
        "    })\n",
        "\n",
        "# Add best state/env config\n",
        "if 'states_envs_df' in locals() and states_envs_df is not None:\n",
        "    best_state_env = states_envs_df.loc[states_envs_df['sharpe'].idxmax()]\n",
        "    master_data.append({\n",
        "        'experiment_type': 'state_env',\n",
        "        'name': f\"Best State/Env: {best_state_env['config']}\",\n",
        "        'total_return': best_state_env['total_return'],\n",
        "        'annualized_return': best_state_env['annualized_return'],\n",
        "        'sharpe': best_state_env['sharpe'],\n",
        "        'max_drawdown': best_state_env['max_drawdown']\n",
        "    })\n",
        "\n",
        "if master_data:\n",
        "    master_df = pd.DataFrame(master_data)\n",
        "    master_df = master_df.sort_values('sharpe', ascending=False)\n",
        "    \n",
        "    print(\"Master Comparison Table (sorted by Sharpe Ratio):\")\n",
        "    print(master_df.to_string(index=False))\n",
        "    \n",
        "    # Visualize master comparison\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "    metrics_to_plot = ['total_return', 'annualized_return', 'sharpe', 'max_drawdown']\n",
        "    \n",
        "    for idx, metric in enumerate(metrics_to_plot):\n",
        "        ax = axes[idx // 2, idx % 2]\n",
        "        master_df_sorted = master_df.sort_values(metric, ascending=False)\n",
        "        bars = ax.barh(master_df_sorted['name'], master_df_sorted[metric])\n",
        "        ax.set_xlabel(metric.replace(\"_\", \" \").title())\n",
        "        ax.set_title(f'{metric.replace(\"_\", \" \").title()} Comparison')\n",
        "        ax.grid(True, alpha=0.3, axis='x')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig(EXPORTS_DIR / \"master_comparison.png\", dpi=150, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    print(f\"Saved master comparison plot to {EXPORTS_DIR / 'master_comparison.png'}\")\n",
        "    \n",
        "    # Export master table\n",
        "    export_table_csv(master_df, EXPORTS_DIR / \"master_comparison.csv\")\n",
        "    export_table_latex(master_df, EXPORTS_DIR / \"master_comparison.tex\", float_format=\"%.4f\")\n",
        "    export_table_html(master_df, EXPORTS_DIR / \"master_comparison.html\", classes='table table-striped')\n",
        "    \n",
        "    # Best of summary\n",
        "    print(\"\\n=== Best Performers ===\")\n",
        "    print(f\"Best Sharpe Ratio: {master_df.loc[master_df['sharpe'].idxmax(), 'name']} ({master_df['sharpe'].max():.4f})\")\n",
        "    print(f\"Best Total Return: {master_df.loc[master_df['total_return'].idxmax(), 'name']} ({master_df['total_return'].max():.4f})\")\n",
        "    print(f\"Lowest Max Drawdown: {master_df.loc[master_df['max_drawdown'].idxmin(), 'name']} ({master_df['max_drawdown'].min():.4f})\")\n",
        "else:\n",
        "    print(\"No data available for master comparison\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
